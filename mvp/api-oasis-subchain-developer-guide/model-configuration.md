---
description: >-
  This section explains how to access and configure a model, create and apply
  configurations via UI or API, and how changes affect the model.
---

# Model Configuration

Developer can create, save, select, change, and apply AI model parameters configuration, including system prompt and settings.

#### Access Configuration:

* Navigate to the 'Playground modeModel Configuration' section.

#### Create and Apply Configurations:

* Use the UI to create new configurations or modify existing ones.
* Apply the desired configuration to your active chat session.
* Configuration must be specifically saved and applied, otherwise previously assigned configuration is applied.
* When changing any parameter of a configuration, including system prompt and name, it can be either updated or a new configuration can be created.\
  New configuration is created either via direct API call 'create\_configuration', or via GUI when Developer clicks 'create & apply' button.
* Developer can see the list of own configurations, and select which one to load (meaning, it will be applied to the model)

Model configuration is a separate entity associated with a user account. Each user has a list of saved configurations, with one configuration always active by default. The model initiates with the default configuration for the specific user, which can be changed at any time.

Configurations are applied dynamically for the user but in a discrete, ordered manner for the model. This means new configuration parameters are applied only when the model finishes processing the current response. The parameters are transmitted along with the request without restarting the model, ensuring that configuration changes do not impact model performance. The exception is the 'system\_prompt' field, which requires the chat to be relaunched when modified.

When modifying any configuration parameter, including the system prompt and name, you have two options: update the current configuration or create a new one.

To update the current configuration, click the 'Update & apply' button. All changes will be saved to the active configuration and applied to the model.

To create a new configuration, you can either make a direct API call using 'create\_configuration' or use the GUI and click the 'create & apply' button. The newly created configuration will be immediately set as active.

## Default configuration

Table 1. List of configuration parameters, descriptions, and available intervals.1

Adjustable model settings provided by our platform:

* **Prompt** \
  Type: text\
  Description: input text  that is provided to the model input to generate a response or perform a task.
* **Name**\
  Type: text\
  &#x20;\- name or unique identifier assigned to a model or model configuration to identify it.&#x20;
* **Model** \
  Type: list of available models\
  Description: parameter that defines the choice of AI model for fine-tuning.\
  \
  Available models: \
  \- Mixtral 8x7b Instruct\

*   **Sampling**\
    Type: on/off button\
    Description: parameter that determines the decoding mechanism. If set to _True_, "sampling" is performed, if _False_ - greedy decoding.\
    \
    _Greedy decoding_ - decoding method in which the most probable word (only one word) is selected at the moment of text generation.\
    _Sampling_ - method in which a certain group of the most probable words is selected, and one of them is randomly selected. This method is currently used more often, as it provides more flexibility to the model.\
    \
    The advantage of greedy decoding is its speed, while the advantage of sampling is the flexibility of the model.

    In simple terms, greedy decoding always selects the most likely word, while sampling selects one of the several most likely words at random. This allows the model to generate more diverse and creative text using sampling, but it may also lead to less predictable results. Greedy decoding, on the other hand, is faster and more predictable, but may not always produce the most interesting or diverse text.\

* **Temperature**\
  Type: decimal\
  Description: parameter that controls the degree of randomness of the output data generated by the model. If the temperature value is close to zero, the model will generate more predictable and deterministic output. If the temperature value is close to 1 or higher, the model will generate more random and varied output data. This parameter can be used to adjust the balance between creativity and predictability of the model output.\

* **Repetition Penalty**\
  Description: parameter to encourage the model to generate more diverse text by "penalizing" the use of duplicate tokens. This can help to reduce repetition in the model's output and increase the diversity of its generated speech.\
  Type: decimal

_Temperature and Repetition Penalty should have different values for optimal results._\
_The optimal value of the Repetition Penalty parameter is 1.2, but you can adjust it at your discretion._

* **Top P**\
  Description: parameter that is used to limit the selection of words to a certain accumulated probability threshold (or until the words run out).\
  Type: decimal\
  \
  For example, if the value of top\_p is 0.5, then the first two words from the previous example will be selected, since 0.3+0.3 is already greater than 0.5. After this stage, the next word will be randomly selected.

These parameters help the model generate more plausible and varied texts by avoiding unlikely or overly predictable words.

## Configuration via REST

Set of methods to fine-tune the model via CLI:

1. GET /config-model - get one config model
2. GET /config-model/all - get all configs model
3. POST /config-model/create - create a new model configuration
4. PUT /config-model/update - update a model configuration
5. DELETE /config-model/ - delete model configuration

### GET /config-model

**Query parameters**:

* apiKey (required)
* configModelId - if you do not pass this parameter, the request will return the current active configuration

**Response body**:

```
{
  "createdAt": "2024-06-14T11:03:24.671Z",
  "updatedAt": "2024-06-14T11:03:24.671Z",
  "deletedAt": "2024-06-14T11:03:24.671Z",
  "id": 0,
  "apiKeyHash": "string",
  "type": {},
  "name": "string",
  "do_sample": {
    "temperature": 0.6,
    "repetition_penalty": 1.2,
    "top_p": 0.9
    "prompt": "string"
  }
}
```

### GET /config-model/all

**Query parameters**:

* apiKey (required)
* configModelId - if you do not pass this parameter, the request will return the current active configuration

**Response body**:

```
{
  "createdAt": "2024-06-14T11:03:24.671Z",
  "updatedAt": "2024-06-14T11:03:24.671Z",
  "deletedAt": "2024-06-14T11:03:24.671Z",
  "id": 0,
  "apiKeyHash": "string",
  "type": {},
  "name": "string",
  "do_sample": {
    "temperature": 0.6,
    "repetition_penalty": 1.2,
    "top_p": 0.9
    "prompt": "string"
  }
}
```

### POST /config-model/create

**Query parameters**

* apiKey (required)
* configModelId - if you do not pass this parameter, the request will return the current active configuration

**Request body**

```
{
  "name": "name",
  "do_sample": {
    "temperature": 0.6,
    "repetition_penalty": 1.2,
    "top_p": 0.9
    "prompt": "string"
  },
  "isActive": false
}
```

**Response body**

```
{
  "createdAt": "2024-06-14T11:03:24.671Z",
  "updatedAt": "2024-06-14T11:03:24.671Z",
  "deletedAt": "2024-06-14T11:03:24.671Z",
  "id": 0,
  "apiKeyHash": "string",
  "type": {},
  "name": "string",
  "do_sample": {
    "temperature": 0.6,
    "repetition_penalty": 1.2,
    "top_p": 0.9
    "prompt": "string"
  }
}
```

### PUT /config-model/update

**Query parameters**

* apiKey (required)
* configModelId - if you do not pass this parameter, the request will return the current active configuration

**Request body**

```
{
  "configModelId": 1,
  "name": "name",
  "do_sample": {
    "temperature": 0.6,
    "repetition_penalty": 1.2,
    "top_p": 0.9
    "prompt": "string"
  },
  "isActive": false
}
```

**Response body**

```
{
  "createdAt": "2024-06-14T11:03:24.671Z",
  "updatedAt": "2024-06-14T11:03:24.671Z",
  "deletedAt": "2024-06-14T11:03:24.671Z",
  "id": 0,
  "apiKeyHash": "string",
  "type": {},
  "name": "string",
  "do_sample": {
    "temperature": 0.6,
    "repetition_penalty": 1.2,
    "top_p": 0.9
    "prompt": "string"
  }
}
```

### PUT /config-model/select-active

**Query parameters**

* apiKey (required)
* configModelId - if you do not pass this parameter, the request will return the current active configuration

**Request body**

```
{
  "configModelId": 1
}
```

**Response body**

```
{
  "createdAt": "2024-06-14T11:03:24.671Z",
  "updatedAt": "2024-06-14T11:03:24.671Z",
  "deletedAt": "2024-06-14T11:03:24.671Z",
  "id": 0,
  "apiKeyHash": "string",
  "type": {},
  "name": "string",
  "do_sample": {
    "temperature": 0.6,
    "repetition_penalty": 1.2,
    "top_p": 0.9
    "prompt": "string"
  }
}
```

### DELETE /config-model/delete

**Query parameters**

* apiKey (required)
* configModelId - if you do not pass this parameter, the request will return the current active configuration

**Response body**

```
{
  "statusCode": 200,
  "message": "Success",
  "data": {}
}

```
